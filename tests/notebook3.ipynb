{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sdan/miniforge3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities: [[0.791958   0.63692826 0.16512099 0.3620784 ]]\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "def transform_query(query: str) -> str:\n",
    "    return f'Represent this sentence for searching relevant passages: {query}'\n",
    "\n",
    "def pooling_np(outputs, attention_mask, strategy='cls'):\n",
    "    if strategy == 'cls':\n",
    "        # Taking the first token (CLS token) for each sequence\n",
    "        return outputs[:, 0]\n",
    "    elif strategy == 'mean':\n",
    "        # Applying attention mask and computing mean pooling\n",
    "        outputs_masked = outputs * attention_mask[:, :, None]\n",
    "        return np.sum(outputs_masked, axis=1) / np.sum(attention_mask, axis=1)[:, None]\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "def cos_sim_np(a, b):\n",
    "    dot_product = np.dot(a, b.T)\n",
    "    norm_a = np.linalg.norm(a, axis=1, keepdims=True)\n",
    "    norm_b = np.linalg.norm(b, axis=1)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_id = 'mixedbread-ai/mxbai-embed-large-v1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)  # Running on CPU\n",
    "\n",
    "# Example documents\n",
    "docs = [transform_query('A man is eating a piece of bread')] + [\n",
    "    \"A man is eating food.\",\n",
    "    \"A man is eating pasta.\",\n",
    "    \"The girl is carrying a baby.\",\n",
    "    \"A man is riding a horse.\",\n",
    "]\n",
    "\n",
    "# Tokenize and process with the model\n",
    "inputs = tokenizer(docs, padding=True, return_tensors='pt')\n",
    "outputs = model(**inputs).last_hidden_state.detach().numpy()  # Convert to NumPy array\n",
    "attention_mask = inputs['attention_mask'].numpy()  # Convert attention mask to NumPy array\n",
    "\n",
    "# Pool embeddings using NumPy\n",
    "embeddings = pooling_np(outputs, attention_mask, 'cls')\n",
    "\n",
    "# Calculate cosine similarities with NumPy\n",
    "similarities_np = cos_sim_np(embeddings[0:1], embeddings[1:])\n",
    "print('Similarities:', similarities_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarities: tensor([[0.7920, 0.6369, 0.1651, 0.3621]])\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "# For retrieval you need to pass this prompt. Please find our more in our blog post.\n",
    "def transform_query(query: str) -> str:\n",
    "    \"\"\" For retrieval, add the prompt for query (not for documents).\n",
    "    \"\"\"\n",
    "    return f'Represent this sentence for searching relevant passages: {query}'\n",
    "\n",
    "# The model works really well with cls pooling (default) but also with mean poolin.\n",
    "def pooling(outputs: torch.Tensor, inputs: Dict,  strategy: str = 'cls') -> np.ndarray:\n",
    "    if strategy == 'cls':\n",
    "        outputs = outputs[:, 0]\n",
    "    elif strategy == 'mean':\n",
    "        outputs = torch.sum(\n",
    "            outputs * inputs[\"attention_mask\"][:, :, None], dim=1) / torch.sum(inputs[\"attention_mask\"])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return outputs.detach().cpu().numpy()\n",
    "\n",
    "# 1. load model\n",
    "model_id = 'mixedbread-ai/mxbai-embed-large-v1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "docs = [\n",
    "    transform_query('A man is eating a piece of bread'),\n",
    "    \"A man is eating food.\",\n",
    "    \"A man is eating pasta.\",\n",
    "    \"The girl is carrying a baby.\",\n",
    "    \"A man is riding a horse.\",\n",
    "]\n",
    "\n",
    "# 2. encode\n",
    "inputs = tokenizer(docs, padding=True, return_tensors='pt')\n",
    "for k, v in inputs.items():\n",
    "    inputs[k] = v\n",
    "outputs = model(**inputs).last_hidden_state\n",
    "embeddings = pooling(outputs, inputs, 'cls')\n",
    "\n",
    "similarities = cos_sim(embeddings[0], embeddings[1:])\n",
    "print('similarities:', similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -41 -118 -100  111   42    0   58  -50  114 -124   59   51  109  -44\n",
      "     5  -67   41  -39  100 -120  -41   48   33   89  -10   -5  -94  -42\n",
      "   -48  -37  -11  100  -21   96   27  127  -27  -49  -84   29 -108   70\n",
      "   103  -37  118   81    4   42   80    8   63   55   -9   30 -118  -19\n",
      "    45   39  -36   61 -120  -86   -6   43]\n",
      " [ -46  -88 -120  124   10   38   54  -62  106   22   27   -5   93  -40\n",
      "    53  -15 -127   77   86   77   -6  -33   55 -126   84   -5   62  -58\n",
      "   -40  -54   38   29   70  -40   15  124   21  -50   -3   29  -94    4\n",
      "   -29 -109   86  -45   36  111  114 -118  127   31  -67 -114 -120   73\n",
      "    46  -10  -36  101    0  115  -52    2]]\n",
      "(2, 64)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.quantization import quantize_embeddings\n",
    "\n",
    "# 1. Load an embedding model\n",
    "model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")\n",
    "\n",
    "# 2. Encode some text and select MRL dimensions\n",
    "mrl_embeddings = model.encode(\n",
    "    [\"Who is german and likes bread?\", \"Everybody in Germany.\"], normalize_embeddings=True)[..., :512] \n",
    "\n",
    "# 3. Apply binary quantization\n",
    "binary_embeddings = quantize_embeddings(mrl_embeddings, precision=\"binary\")\n",
    "print(binary_embeddings)\n",
    "print(binary_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Represent this sentence for searching relevant passages: A man is eating a piece of bread', 'A man is eating food.', 'A man is eating pasta.', 'The girl is carrying a baby.', 'A man is riding a horse.']\n",
      "Hamming distances: [57 62 63 64]\n",
      "Normalized scores: [0.88867188 0.87890625 0.87695312 0.875     ]\n",
      "binplus Hamming distances: [57 62 63 64]\n",
      "binplus Normalized scores: [0.88867188 0.87890625 0.87695312 0.875     ]\n",
      "int similarities: tensor([[0.7920, 0.6369, 0.1651, 0.3621]])\n",
      "query_docs similarities: tensor([[0.7920, 0.6369, 0.1651, 0.3621]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "# 1. load model\n",
    "model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")\n",
    "\n",
    "# For retrieval you need to pass this prompt.\n",
    "query = 'Represent this sentence for searching relevant passages: A man is eating a piece of bread'\n",
    "\n",
    "docs = [\n",
    "    \"A man is eating food.\",\n",
    "    \"A man is eating pasta.\",\n",
    "    \"The girl is carrying a baby.\",\n",
    "    \"A man is riding a horse.\",\n",
    "]\n",
    "\n",
    "docs_plus = [query] + docs\n",
    "print(docs_plus)\n",
    "\n",
    "# 2. Encode\n",
    "docs_embeddings = model.encode(docs, normalize_embeddings=True)[..., :512]\n",
    "query_embedding = model.encode(query, normalize_embeddings=True)[..., :512]\n",
    "\n",
    "docs_binary_embeds = np.packbits(docs_embeddings > 0, axis=-1)\n",
    "query_binary_embed = np.packbits(query_embedding > 0, axis=-1)\n",
    "\n",
    "hamming_distances = np.sum(docs_binary_embeds != query_binary_embed, axis=1)\n",
    "max_distance = docs_binary_embeds.shape[1] * 8  # Maximum possible Hamming distance\n",
    "scores = 1 - hamming_distances / max_distance\n",
    "# docs_embeddings = model.encode(docs, normalize_embeddings=True)[..., :512] \n",
    "# query_embedding = model.encode(query, normalize_embeddings=True)[..., :512]\n",
    "\n",
    "# docs_binary_embeds = (np.packbits(docs_embeddings > 0).reshape(docs_embeddings.shape[0], -1) - 128).astype(np.int8)\n",
    "# query_binary_embed = (np.packbits(query_embedding > 0).reshape(query_embedding.shape[0], -1) - 128).astype(np.int8)\n",
    "\n",
    "# 3. calculate similarities with hamming distance between binary embeddings\n",
    "# Calculate the Hamming distances\n",
    "# hamming_distances = np.sum(docs_binary_embeds != query_binary_embed, axis=1)\n",
    "\n",
    "# Convert Hamming distances to similarities (optional)\n",
    "# similarities = -hamming_distances\n",
    "\n",
    "print('Hamming distances:', hamming_distances)\n",
    "print('Normalized scores:', scores)\n",
    "\n",
    "#use transform_query with binary embeddings\n",
    "docsplus_embeddings = model.encode(docs_plus, normalize_embeddings=True)[..., :512]\n",
    "docsplus_binary_embeds = np.packbits(docsplus_embeddings > 0, axis=-1)\n",
    "hamming_distances = np.sum(docsplus_binary_embeds[1:] != docsplus_binary_embeds[0], axis=1)\n",
    "max_distance = docsplus_binary_embeds.shape[1] * 8  # Maximum possible Hamming distance\n",
    "scores = 1 - hamming_distances / max_distance\n",
    "\n",
    "print('binplus Hamming distances:', hamming_distances)\n",
    "print('binplus Normalized scores:', scores)\n",
    "\n",
    "# USE NORMAL EMBS\n",
    "docs_int_embeddings = model.encode(docs)\n",
    "query_int_embedding = model.encode(query)\n",
    "\n",
    "similarities = cos_sim(query_int_embedding, docs_int_embeddings)\n",
    "print('int similarities:', similarities)\n",
    "\n",
    "# USE WITH QUERY   \n",
    "querydocs_embedding = model.encode(docs_plus)\n",
    "similarities = cos_sim(querydocs_embedding[0], querydocs_embedding[1:])\n",
    "print('query_docs similarities:', similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
